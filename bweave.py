# -*- coding: utf-8 -*-
"""bweave.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GzewGmsYZTdmSP-QzlgsKrEhcviKGx6e
"""

#Import necessary modules
import datetime
import pandas as pd
 
#Files pings.csv,cust.csv,test.csv are stored in Google drive
#Mount drive and provide auth to access
from google.colab import drive
drive.mount("/content/drive")

"""Read files from google drive """

print("Reading files>>>>>>")
pings = pd.read_csv("/content/drive/My Drive/bw/pings.csv")
print("Shape of pings.csv file",pings.shape)
print("Reading files complete>>>")

#Convert Unix timestamp column to datetime tyoe
pings['timestamp']=pd.to_datetime(pings['timestamp'],unit='s')

#Add date column from timestamp
pings['date']=pings['timestamp'].dt.date
#Add time column from timestamp
pings['time']=pings['timestamp'].dt.time

#Drop timestamp column
pings=pings.drop(columns=['timestamp'])

#Group the pings data by Customer and Date to calculate duration
#spent on app by customer for a day
grouped = pings.groupby(['id','date'])

#New Dataframe to append day wise customer wise duration spent on app
new_pings = pd.DataFrame(columns=['id','date','dura'])

#Global duration variable
dur = datetime.timedelta(0)

#Function to calculate duration spent by customer on app per day
def compdur(df):
    global dur
    pre = str(df['time'][0:1])
    pre=pre.split('\n')
    pre=pre[0].split()
    pre=pre[1]
    for i,r in df.iterrows():
        x = str(r['time'])	
        dif = ( datetime.datetime.strptime(x,"%H:%M:%S") - datetime.datetime.strptime(pre,"%H:%M:%S"))
        #If difference betn successive values is more than 5 mins,
        #do not include that difference. The current ping will be 
        #treated as new login.
        if dif < datetime.timedelta(0,300):
           dur = dur + dif
        pre = str(r['time'])
    return dur

print("Computing duration spent on App by Customer by day>>>>>")
#Pass every group to function compdur to calculate duration spent on app
#for the day by the customer and append to a new dataframe
#This will result in a dataframe with a customer, corresponding date
# and duration spent on app in hours

for i,g in grouped:
        cust_id = str(g['id'][0:1])
        cust_id = cust_id.split('\n')
        cust_id = cust_id[0].split()
        cust_id = cust_id[1]
        
        date    = str(g['date'][0:1])
        date = date.split('\n')
        date = date[0].split()
        date = date[1]
        
        dur = datetime.timedelta(0)
        res = compdur(g).total_seconds()
	#Convert to hours
        res = res/3600
        new_pings = new_pings.append({'id':int(cust_id),'date':date,'dura':res},ignore_index=True)

print("Dataframe with customer duration per day")
print(new_pings.describe)

new_pings['id']=pd.to_numeric(new_pings['id'])
new_pings['date'] = pd.to_datetime(new_pings['date'])

#Extracting day from date column and creating a new 'day' column
new_pings['day']=new_pings['date'].dt.day_name()

#Save the new_pings dataframe as csv file in drive
#the previous section takes a very long time.

new_pings.to_csv('/content/drive/My Drive/bw/new_pings.csv',index=False)

#Temporary section to read new_pings.csv from google drive 
#Not necessary to run this in sequence. Further steps can be started from this section
#in case the runtime disconnects.
new_pings = pd.read_csv('/content/drive/My Drive/bw/new_pings.csv')

#Read Customer file from Google drive
cust = pd.read_csv('/content/drive/My Drive/bw/customers.csv')

#On analysing Customers.csv, it was found to contain duplicates for 3
#customer ids. Drop dupicate customer ids
cust.drop_duplicates(subset=['id'],inplace=True)

#Consolidating data from customers.csv and new_pings
df_cons = pd.merge(new_pings,cust,how='inner',on='id')
print("Shape of consolidated file",df_cons.shape)
df_cons['dura'] = df_cons['dura'].round(1)
df_cons.head()

#Importing necessary methods from sklearn for model building
#Segregating data into X and y for training
import sklearn
from sklearn import tree,linear_model,ensemble,neighbors
X = df_cons[['id','gender','age','number_of_kids','day']]

#Encoding categorical column 'gender'
gend = pd.get_dummies(X['gender'], drop_first=False)
X = pd.concat([X,gend],axis=1)
X.drop(columns=['gender'],inplace=True)

#Encoding categorical column 'day'
days = pd.get_dummies(X['day'], drop_first=False)
X=pd.concat([X,days],axis=1)
X.drop(columns=['day'],inplace=True)

#Columns in input datframe after categorical encoding
print("Columns in X after categorical encoding",X.columns)
#Storing target variable in separate dataframe
y = df_cons[['dura']]

#Instantiating the model
#model = tree.DecisionTreeRegressor(max_depth=5)
#model = ensemble.AdaBoostRegressor(tree.DecisionTreeRegressor(max_depth=3),n_estimators=120,learning_rate=0.95)
#model = linear_model.LinearRegression()
model = neighbors.KNeighborsRegressor(n_neighbors=5,weights='uniform')

#Read test file from Google drive
test = pd.read_csv('/content/drive/My Drive/bw/test.csv')

test['date'] = pd.to_datetime(test['date'])

#Actual values from test file
act=test['online_hours']

#Drop the target variable from test
test.drop(columns=['online_hours'],inplace=True)

#Add day column in test dataframe and merge columns from
#customer.csv
test['day']=test['date'].dt.day_name()
test = pd.merge(test,cust,how='inner',on='id')
test.drop(columns=['date'],inplace=True)

#Rearranging columns
test = test[['id','gender','age','number_of_kids','day']]

#Encoding categorical column 'gender'
gend = pd.get_dummies(test['gender'], drop_first=False)
test= pd.concat([test,gend],axis=1)
test.drop(columns=['gender'],inplace=True)

#Encoding categorical column 'day'
days = pd.get_dummies(test['day'], drop_first=False)
test=pd.concat([test,days],axis=1)
test.drop(columns=['day'],inplace=True)

#Train the model with dataframe 
from sklearn import metrics
model.fit(X,y)
pred = model.predict(test)
print("RMSE:",metrics.mean_squared_error(act,pred,squared=False).round(2))
print(act.head())
pred=pred.round(1)
print(pred[0:5])
pred=pd.DataFrame(pred)
#Save actual and predicted values to files
pred.to_csv('/content/drive/My Drive/bw/pred.csv')
act.to_csv ('/content/drive/My Drive/bw/act.csv')